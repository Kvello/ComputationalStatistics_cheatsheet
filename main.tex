\documentclass[10pt]{article} % Use extarticle class for smaller font sizes
\usepackage[a4paper, landscape, margin=0.1cm]{geometry}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\setlength{\columnseprule}{0.5pt}
\begin{document}
\begin{multicols}{4}
    \vbox{
        \begin{flushleft}
            {\Large \textbf{ST4231 Cheat-sheet}}
            \vspace{0.1cm}
            \hrule
            \vspace{0.1cm}
            \textbf{Basics relationships}
            \begin{align*}
                \text{Var}[X]   & = E[(X-E[X])^2]       \\
                                & = E[X^2]-E[X]^2       \\
                \text{Cov}[X,Y] & = E[(X-E[X])(Y-E[Y])] \\
                                & = E[XY]-E[X]E[Y]
            \end{align*}
            \hrule
            \textbf{Inversion method}
            \begin{equation*}
                T(X) = F^{(-1)}(U) \Rightarrow T(X) \sim f(x)
            \end{equation*}
            \textbf{Generalized inverse}
            \begin{equation*}
                F^{(-1)}(U) = \inf\{z \in \mathcal{R} :F(z)\geq u\}
            \end{equation*}
            \hrule
            \vspace{0.1cm}
            \textbf{Change-of-variable formula}\\
            Suppose \(g(x)\) is one-to-one and
            \(\mathcal{C}^0\), \(X \sim f_X(x)\),
            \(Y = g(X)\), then
            \[
                f_Y(y) = f_X(g^{-1}(y))\left|\frac{dg^{-1}(y)}{dy}\right|
            \]
            \hrule
            \vspace{0.1cm}
            \textbf{Central Limit Theorem}
            \(E[X]<\infty\) then the follwing convergence
            in distribution holds
            \begin{equation*}
                \lim_{n\to \infty}\sqrt{n}(\bar{X_n}-\mu) = \mathcal{N}(0,\sigma^2)
            \end{equation*}
            \hrule
            \vspace{0.1cm}
            \textbf{Fundamantal theorem of sampling}\\
            If \(X\) is a random variable with pdf \(f(x)\), then
            simulating \(X\) is equivalent to simulating a pair of
            variables \((U,X)\) jointly from
            \begin{equation*}
                (X,U) \sim \text{Uniform}\{(x,u): 0 < u < f(x)\}
            \end{equation*}
            \hrule
            \vspace{0.1cm}
            \textbf{Rejection Sampling Algorithm}
            Suppose \(f(x)= c \tilde{f(x)}\) where \(\tilde{f(x)}\) is
            known and \(c\) is not. \(\tilde{f}(x)\leq \tilde{M}g(x)\; \forall x\)
            \begin{enumerate}
                \item Generate \(Y\sim G\)
                \item Generate \(U \sim \text{Uniform}[0,1]\)
                \item If \(U \leq \frac{\tilde{f}(Y)}{\tilde{M}g(Y)}\), then accept: set \(X=Y\)
                      Otherwise reject: return to step \((1)\)
            \end{enumerate}
        \end{flushleft}
    }
    \columnbreak
    \vbox{
    \textbf{Basic distributions}\\
    {\small \textbf{Uniform}}\\
    \(
    f(x;a,b) = \begin{cases}
        \frac{1}{b-a}, \; \text{if } 0\leq x \leq b \\
        0 \; \text{otherwise}
    \end{cases} \)\\
    \(
    E[X] = \frac{a+b}{2}\;Var[X] = \frac{(b-a)^2}{12}
    \)\\
    {\small \textbf{Binomial}}\\
    \(
    f(x;n,p) = {n \choose x} p^{x} (1-p)^{n-x}\; x=0,1\dots,n \)\\
    \(
    E[X] =np\; Var[X] = np(1-p)
    \)\\
    {\small \textbf{Geometric}}\\
    \(
    f(x;p)  = p(1-p)^{x-1}
    \) \\
    \(
    E[X] = \frac{1}{p}\;Var[X] = \frac{1-p}{p^2}
    \)\\
    {\small \textbf{Poisson}}\\
    \(
    f(x)= \frac{\mu^x}{x!}e^{-\mu},\; x = 0,1,2,\dots
    \)\\
    \(
    E[X] = \mu,\; \text{Var}[X] = \mu
    \)\\
    {\small \textbf{Hypergeometric}}\\
    \(
    f(x;N,n,k) = \frac{{k \choose x}{N-k \choose n-x}}{{N \choose n}},\; x = 0,1,\dots \min(n,k)\\
    E[X] = \frac{nk}{N},\; \text{Var}[X] = \frac{N-n}{N-1}\frac{nk}{N}(1-\frac{k}{N})
    \)\\
    {\small \textbf{Negative Binomial}}\\
    \(
    f(x;k,p) = {x-1 \choose k-1}p^k(1-p)^{x-k},\; x = 1,2,\dots
    \)\\
    \(
    E[X] = \frac{k}{p},\; Var[X] = \frac{1-p}{p^2}k
    \)\\
    {\small \textbf{Multinomial}}\\
    \(
    f(x_1,\dots,x_k;p_1,\dots,p_k,n) = \frac{n!}{x_1!\hdots x_k}p_1^{x_1}\hdots p_k^{x_k}
    \)\\
    \(
    E[X_i] = np_i,\; \text{Var}[X_i] = np_i(1-p_i)
    \)\\
    \(
    \text{Cov}(X_i,X_j) =-np_ip_j
    \)\\
    {\small \textbf{Exponential distribution}}\\
    \(
    \lambda e^{-\lambda x},\; x\geq 0
    \)\\
    \(
    E[X] = \frac{1}{\lambda},\; \text{Var}[\frac{1}{\lambda^2}]
    \)\\
    {\small \textbf{\(\mathcal{X}^2\) distribution}}\\
    {\small
    \(
    f(x) = \frac{1}{2^{\nu/2}\Gamma(\nu/2)}x^{\nu/2-1}e^{-x/2},\; x \geq 0,\; \nu = 1,2,\dots
    \)
    }\\
    \(
    E[X] = \nu,\; \text{Var}[X] = 2\nu
    \)\\
    {\small \textbf{Gamma}}\\
    \(
    f(x)=\frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1}e^{-x\beta}, \; x \geq 0
    \)\\
    \(
    E[X] = \alpha\beta, \text{Var} = \alpha\beta^2
    \)\\
    {\small \textbf{Weibull}}\\
    \(
    f(x) = \alpha\beta x^{\beta-1}e^{-\alpha x^{\beta}},\; x \geq 0
    \)\\
    {\small
    \(
    E[X] = \alpha^{-1/\beta}\Gamma(1+1/\beta)
    \)\\
    \(
    \text{Var}[X] = \alpha^{2/\beta}\left[\Gamma(1+\frac{2}{\beta})-\Gamma(1+\frac{1}{\beta})^2\right]
    \)
    }\\
    {\small \textbf{Beta}}\\
    \(
    f(x) = \frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1},\; 0 \leq x \leq 1
    \)\\
    \(
    E[X] = \frac{\alpha}{\alpha+\beta},\; \text{Var}[X] = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
    \)\\
    {\small \textbf{Normal}}\\
    \(
    f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}},\; -\infty < x < \infty
    \)\\
    \(
    E[X] = \mu,\; \text{Var}[X] = \sigma^2
    \)\\
    {\small \textbf{T-distribution}}\\
    \(
    f(x) = \frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}\left(1+\frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}},\; -\infty < x < \infty
    \)\\
    \(
    E[X] = 0,\;\nu \geq 2 \;\text{Var}[X] = \frac{\nu}{\nu-2}
    \)
    }
    \columnbreak
    \vbox{
        \begin{flushleft}

            \textbf{Box-Muller v1}
            \begin{enumerate}
                \item \(U_1,U_2 \overset{\text{i.i.d.}}{\sim} \text{Uniform}(0,1)\)
                \item \(R = \sqrt{-\ln(U_1)}\), \(\theta = 2\pi U_2\)
                \item \(X = R\cos(\theta)\), \(Y = R\sin(\theta)\)
            \end{enumerate}
            \textbf{Box-Muller v2}
            \begin{enumerate}
                \item \(U_1,U_2 \overset{\text{i.i.d.}}{\sim} \text{Uniform}(0,1)\)
                \item \(V_1 = 2U_1-1\),\(V_2 = 2U_2-1\),\(S=V_1^2+V_2^2\)
                \item If \(S>1\)return to step 1(Rejection sampling)
                \item Return \(X=\sqrt{-2\ln(S)/S} \cdot V_1\), \\
                      \(Y=\sqrt{-2\ln(S)/S} \cdot V_2\)
            \end{enumerate}
            \textbf{General Multivariate Normal}\\
            To generate d-dimentional normal with mean \(\mu\) and covariance matrix
            \(\Sigma\):
            \begin{enumerate}
                \item Generate \(\mathbf{Z} = (Z_1,\dots,Z_d)^{\top}\)
                \item Set \(X=\mathbf{LZ} + \boldsymbol{\mu}\)
            \end{enumerate}
            Where \(\mathbf{L}\) satisfies \(\mathbf{LL^{\top}=\Sigma}\)(Cholesky)
            \hrule
            \vspace{0.1cm}
            \textbf{Simple sampling}\\
            Estimate \(\theta = E[\phi(X)]\)
            {\small
                    \begin{align*}
                        \hat{\theta}   & = \frac{1}{n}\sum_{i=1}^{n}\phi(X_i),\;\text{Var}(\theta) = \frac{\overbrace{\int_{\mathcal{S}}\phi(x)f(x)dx-\theta^2}^{\text{asymp. variance}}}{n} \\
                        \hat{\sigma^2} & = \frac{1}{n} \sum_{i=1}^{n}\phi^2(X_i)-\hat{\theta}^2, \;\hat{\theta} \pm 1.96 \frac{\hat{\sigma^2}}{\sqrt{n}}
                    \end{align*}
                }
            \hrule
            \vspace{0.1cm}
            \textbf{Importance sampling}\\
            {\small
            \begin{align*}
                \hat{\theta}_{IS}  & = \frac{1}{n}\sum_{i=1}^{n}\frac{\phi(X_i)f(Y_i)}{g(Y_i)}                                                                                \\
                \text{Var}(\theta) & = \frac{\overbrace{\int_{\mathcal{S}}\frac{\phi^2(x)f^2(x)}{g(x)}dx - \theta^2}^{\text{asymp. variance}}}{n}                             \\
                \hat{\sigma^2}     & = \frac{1}{n} \sum_{i=1}^{n}\phi^2(X_i)\frac{f^2(X_i)}{g^2(X_i)}-\hat{\theta}^2, \;\hat{\theta} \pm 1.96 \frac{\hat{\sigma^2}}{\sqrt{n}}
            \end{align*}
            }\\
            \textit{Optimal }\(g(x)\): \(g(x) \propto |\phi(x)|\cdot f(x)\)
        \end{flushleft}
    }
    \columnbreak
    \vbox{
        \begin{flushleft}
            \textbf{Self-normalized Importance Sampling}\\
            \(\tilde{w}(x) = \frac{\tilde{f}(x)}{\tilde{g}(x)}\; \forall x \in \mathcal{S}\),
            \(\hat{\theta}_{SIS} = \frac{\sum_{i=1}^n\phi(X_i)\tilde{w}(X_i)}{\sum_{i}^n\tilde{w}(X_i)}\)\\
            \(E_f\left[\hat{\theta}_{SIS}\right] \not= \theta, \; \text{bias}(\hat{\theta}_{SIS}) = \mathcal{O}(1/n)\),
            \(\text{fluctuation}(\hat{\theta}_{SIS}) = \mathcal{O}(1/\sqrt{n})\)\\
            \(\frac{\hat{\sigma}_{SIS}^2}{n} = \frac{\sum_{i=1}^n\left\{\tilde{w}^2(X_i)\left[\phi(X_i)-\hat{\theta}_{SIS}\right]^2\right\}}{\left\{\sum_{i=1}^n\tilde{w}(X_i)\right\}^2}\)\\
            95\% asymp. conf. interval: \(\hat{\theta}_{SIS} \pm 1.96\sqrt{\frac{\hat{\sigma}^2_{SIS}}{n}}\)
            \hrule
            \vspace{0.1cm}
            \textbf{Control Variates}\\
            Suppose we know:
            \begin{enumerate}
                \item an unbiased estimator \(\hat{h}\) of \(E[h(X)]\)
                \item \(E_f[h(X)]\) and \(\text{Var}[\hat{h}]\)
                \item the value or sign of \(\text{Cov}(\hat{theta},\hat{h})\)
            \end{enumerate}
            Let \(\tilde{\theta} = \hat{\theta} + \beta\left\{\hat{h} - E_f[h(X)]\right\}\), then
            \(\text{Var}(\tilde{\theta}) = \text{Var}(\hat{\theta}) + \beta^2 \text{Var}(\hat{h}) + 2 \beta\text{Cov}(\hat{\theta},\hat{h})\)\\
            which is minimized when \(\beta = -\frac{\text{Cov}(\hat{\theta},\hat{h})}{\text{Var}(\hat{h})}\)\\
            The corresponding smallest value is  \(\text{Var}(\tilde{\theta}) = (1-\rho^2_{\hat{\theta},\hat{h}})\text{Var}(\hat{\theta}), \; \rho^2_{\hat{\theta},\hat{h}} = \text{Cor}(\hat{\theta},\hat{h})\)
            \hrule
            \vspace{0.1cm}
            \textbf{Antithetic Variates Method}\\
            If \(g(x)\) is a monotone function then\\
            \([g(u_1)-g(u_2)][g(1-u_1)-g(1-u_2)] \leq 0\)
            From this we can show that if \(X = F^{-1}(U_1),\;X' = F^{-1}(1-U_1)\)\\
            Then \(2\text{Cov}(X,X') \leq 0\) which in turn implies that
            \(\text{Var}\left(\frac{X+X'}{2}\right) \leq \frac{1}{2}\text{Var}(X)\)
            \hrule
            \vspace{0.1cm}
            \textbf{Rao-Blackwellization}\\
            \(
            \hat{\theta}_{RB} = \frac{1}{N}\sum_{i=1}^N E[\phi(X_i)|Y=Y_i]
            \)\\
            \(\hat{\theta}_{RB}\) is unbiased, and reduces the variance compared to
            simple sampling, by the law of total variance:\\
            \(
            \text{Var}[\hat{\theta}] = \frac{1}{N}\text{Var}[\phi(X)]
            \)\\
            \(
            = \frac{1}{N}\left(\text{Var}[E[\phi(X)|Y]] + E[\text{Var}[\phi(X)|Y]]\right)
            \)\\
            \(
            \geq \frac{1}{N}\text{Var}[E[\phi(X)|Y]]
            \)
            \vspace{0.1cm}
            \hrule
            \vspace{0.1cm}
            \textbf{EM-Algorithm}
            For latent variable model:\\
            \textbf{E}-step:\\
            \(Q(\theta|\theta^{(k)}) = E_{Z}[l^{C}(\theta;Y,Z)|Y,\theta^{(k)}]\)\\
            \textbf{M}-step:\\
            \(\theta^{(k+1)} = \arg \max_{\theta \in \Theta}Q(\theta|\theta^{(k)})\)\\
        \end{flushleft}
    }
    \columnbreak
    \vbox{
        \begin{flushleft}
            \textbf{Metropolis-Hastings Algorithm}\\
            \begin{enumerate}
                \item Set \(\theta^{(0)}\) to some initial value
                \item \textbf{for} \(t=0\) to \(T-1\) \textbf{do}:
                      \begin{enumerate}
                          \item Generate \(\theta^*\) from \(q(\theta^*|\theta^{(t)})\)
                          \item Compute the acceptance probability:
                                \(\alpha(\theta^*,\theta^{(t)}) = \min\left\{1,\frac{p(Y|\theta^*)\cdot \pi(\theta^*)\cdot Q(\theta^*,\theta^{(t)})}{p(Y|\theta^{(t)})\cdot \pi(\theta^{(t)}) \cdot Q(\theta^{(t),\theta^*})}\right\}\)
                      \end{enumerate}
                \item Generate \(U \sim \text{Uniform}(0,1)\)
                \item If \(U \leq \alpha(\theta^*,\theta^{(t)})\) then \(\theta^{(t+1)} = \theta^*\)
                \item Otherwise \(\theta^{(t+1)} = \theta^{(t)}\)
            \end{enumerate}
            \vspace{0.1cm}
            \hrule
            \vspace{0.1cm}
            \textbf{Gibbs-sampler}\\
            \begin{enumerate}
                \item Initialize \(\theta^{(0)} = (\theta_1^{(0)},\dots,\theta_d^{(0)})^{\top}\)
                \item \textbf{for} \(t=0\) to \(T-1\) \textbf{do}:
                      \begin{enumerate}
                          \item Sample \(\theta_1^{(t+1)}\) from \(p(\theta_1|\theta_2^{(t)},\dots,\theta_d^{(t)},Y)\)
                          \item Sample \(\theta_2^{(t+1)}\) from \(p(\theta_2|\theta_1^{(t+1)},\theta_3^{(t)},\dots,\theta_d^{(t)},Y)\)
                          \item \(\vdots\)
                          \item Sample \(\theta_d^{(t+1)}\) from \(p(\theta_d|\theta_1^{(t+1)},\dots,\theta_{d-1}^{(t+1)},Y)\)
                      \end{enumerate}
                \item Set \(\theta^{(t+1)} = (\theta_1^{(t+1)},\dots,\theta_d^{(t+1)})\)
            \end{enumerate}
            \vspace{0.1cm}
            \hrule
            \vspace{0.1cm}
        \end{flushleft}
    }
    \columnbreak
    \vbox{
        \begin{flushleft}
            {\large \textbf{Markov chains}}\\
            A Markov chain is \textit{irreducible} if all states consist
            of a single class. Meaning all states are accessible
            from each other.\\
            An \textit{irreducible} Markov chain \(X\) is recurrent if
            \(P[\tau_{ii} < \infty] = 1\) for all states,
            where \(\tau_ii = \min\{t > 0: X_t =i| X_0 = i\}\)\\
            An \textit{irreducible} \textit{recurrent} Markov chain \(X\)
            is \textit{positive recurrent} if \(E[\tau_{ii}] < \infty\)
            for all states. Otherwise it is \textit{null recurrent}.\\
            If a Markov chain only has a finite number of states, and
            is \textit{irreducible} then it must be positive recurrent.\\
            \textit{positive reccurent} \(\Leftrightarrow\) there exists stationary
            pmf \(\boldsymbol{\pi}(\cdot)\).\\
            An \textit{irreducible} chain in called \textit{aperiodic}
            if for some and hence all \(i\), the greatest common divider of
            \(\{t: p_{ii}>0\} = 1\)\\
            The \textit{stationary distribution} satisfies \(\boldsymbol{\pi} P=\boldsymbol{\pi}\)\\
            If a Markov chain is \textit{irreducible} and \textit{aperiodic}, then it has a unique \textit{stationary distribution}\\
            A \textit{closed class} is one that is impossible to leave, so \(p_{ij} = 0,\;\text{if } i \in C,\; j \not \in C\)\\
            A Markov chain is \textit{reversible} if it satisfies Kolmogorov's criterion:\\
            \(
            \pi_i p_{ij} = \pi_j p_{ji},\; \forall i,j
            \)\\

            %\textbf{Chapman-Kolmogorov equations}\\
            %\(
            %p_{ij}(t+s) = \sum_{k=1}^K p_{ik}(t)p_{kj}(s)
            %\)\\
            %\(
            %p_{ij}(t) = \sum_{k=1}^K p_{ik}(t)p_{kj}(0)
            %\)\\
            %\(
            %p_{ij}(t) = \sum_{k=1}^K p_{ik}(0)p_{kj}(t)
            %\)\\
            \textbf{Convergence Theorem}\\
            Let \(X = \{X_1,X_2,\dots\}\) be a \textit{stationary} \& \textit{reccurent} Markov chain with the transition
            matrix \(P\) and transition probabilities \(p_{ij}\) from any
            state \(i\) to state \(j\). Then
            \begin{enumerate}
                \item the stationary distribution is the unique distribution satisfying \(\sum_{i} \pi_i p_{ij}(t) = \pi_i \forall j \forall t \geq 0\)
                \item if \(E_{\pi}[|h(X)|]<\infty\) then \(\lim_{N \to \infty} N^{-1}\sum_{k=1}^N h(X_K) = E_{\pi}[h(X)]\)
                \item if \(E_{\pi}[\phi^2(X)]<\infty\) then {\tiny \(\lim_{T\to \infty} \left(\frac{1}{\sqrt{T}}\sum_{t=1}^T\phi(X_t)-E_{\pi}[\phi(X)]\right) \to \mathcal{N}(0,\sigma^2)\)}
                \item if X is \textit{aperiodic} then \(p_{ij}(t) \to \pi_j\) as \(t\to \infty\) for all \(i,j\)
            \end{enumerate}

            \textbf{Detailed balance condition}\\
            Holds if the Markov chain is \textit{reversible}\\
            \(\exists x_j, j=1,2,\dots,K : x_ip_{ij} = x_jp_{ji} \forall i\not=j, \sum_{j=1}^K x_j = 1 \Rightarrow \pi_j \propto x_j\)

        \end{flushleft}
    }
    \columnbreak
    \vbox{
        \begin{flushleft}
            {\large\textbf{Common problems}}\\
            \textbf{Inversion method}\\
            \(U \sim \text{Uniform}(0,1)\)\\
            \(X = -\frac{1}{\lambda}\ln(U) \Rightarrow X \sim \text{Exp}(\lambda)\)\\
            \(X = (-\frac{1}{\alpha}\ln(U))^{1/p},\; X \sim \text{Weibull}(\alpha,\beta)\)\\
            \textbf{Other methods}\\
            {\small
            \(Y_1,\dots Y_\alpha \overset{\text{i.i.d}}{\sim} \text{Exp}(\beta),\; \Rightarrow \sum Y_i \sim \text{Gamma}(\alpha,\beta)\)
            }\\
            {\small
            \(
            U_1,\dots, U_{\beta+\alpha-1} \overset{\text{i.i.d}}{\sim} \text{Unif}(0,1) \Rightarrow U_{(\alpha)} \sim \text{Beta}(\alpha,\beta)
            \)}\\
            \textbf{EM-algorithm}\\
            Mixture of normals:\\
            \(
            y_i \sim p \mathcal{N}(\mu_1,\sigma_1^2) + (1-p)\mathcal{N}(\mu_2,\sigma_2^2)
            \)\\
            Let\\
            \(
            f_i^{(k)}(x) = \frac{1}{\sqrt{2\pi(\sigma_i^{2})^2}}\exp\left\{-\frac{(x-\mu_i^{(k)})^2}{2(\sigma_i^{2})^{(k)}}\right\},\; i = 1,2
            \)\\
            \(
            \alpha_i^{(k,1)} = \frac{p^{(k)}f_1(y_i)}{p^{(k)}f_1(y_i) + (1-p^{(k)})f_2(y_i)}
            \)\\
            \(
            \alpha_i^{(k,2)} = 1- \alpha_i^{(k,1)}
            \)\\
            \(
            \mu_1^{(k+1)} = \frac{\sum_{i=1}^N \alpha_i^{(k,1)y_i}}{\sum_{i=1}^{N}\alpha_i^{(k,1)}},\;\mu_2^{(k+1)} = \frac{\sum_{i=1}^N \alpha_i^{(k,2)y_i}}{\sum_{i=1}^{N}\alpha_i^{(k,2)}}
            \)\\
            \(
            (\sigma_1^2)^{(k+1)} = \frac{\alpha_i^{(k,1)}(y_i-\mu_1^{(k+1)})^2}{\sum_{i=1}^{N}\alpha_i^{(k,1)}}
            \)\\
            \(
            (\sigma_2^2)^{(k+1)} = \frac{\alpha_i^{(k,2)}(y_i-\mu_2^{(k+1)})^2}{\sum_{i=1}^{N}\alpha_i^{(k,2)}}
            \)\\
            \(
            p^{(k+1)} = \frac{\sum_{i=1}^{N}\alpha_i^{(k,1)}}{N}
            \)\\
            Mixtures of Poissons:\\
            \(
            y_i \sim p \text{Poisson}(\lambda_1) + (1-p)\text{Poisson}(\lambda_2)
            \)\\
            Let:\\
            \(
            f_i^{(k)}(x) = \frac{\lambda_i^{y_i}}{y_i!}\exp\{-\lambda_i\},\; i = 1,2
            \)\\
            \(
            \alpha_i^{(k,1)} = \frac{p^{(k)}f_1^{(k)}}{p^{(k)}f_1^{(k)} + (1-p^{(k)})f_2^{(k)}},\; \alpha_i^{(k,2)} = 1-\alpha_i^{(k,1)}
            \)\\
            \(
            \lambda_1^{(k+1)} = \frac{\sum_{i=1}^N \alpha_i^{(k,1)}y_i}{\sum_{i=1}^N \alpha_i^{(k,1)}},\; \lambda_2^{(k+1)} = \frac{\sum_{i=1}^N \alpha_i^{(k,2)}y_i}{\sum_{i=1}^N \alpha_i^{(k,2)}}
            \)\\
            \(
            p^{(k+1)} = \frac{\sum_{i=1}^N \alpha_i^{(k,1)}}{N}
            \)
        \end{flushleft}
    }
    \columnbreak
    \vbox{
        \begin{flushleft}
            {\large \textbf{Misc.}}\\
            \textbf{Divergence/Convergence of integrals of rational functions}\\
            \(
            \int_1^\infty \frac{1}{x^p} = \begin{cases}
                < \infty ,\; p>1 \\
                \to \infty ,\; p \leq 1
            \end{cases}
            \)\\
            \(
            \int_0^a \frac{1}{x^p} = \begin{cases}
                < \infty ,\; p<1 \\
                \to \infty ,\; p \geq 1
            \end{cases}
            \)\\
            \textbf{Convex function}\\
            \textbf{Some integrals}\\
            \(
            \int
            \)
            \textbf{Trigonometric identeties}\\
        \end{flushleft}
    }
\end{multicols}
\end{document}
